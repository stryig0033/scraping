{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30688,"status":"ok","timestamp":1703389312444,"user":{"displayName":"Tomoki Takata","userId":"02827097188944657983"},"user_tz":-540},"id":"D5pE0g3m6AGF","outputId":"26dfc2d8-1bf2-40c2-d6a3-6dcbd8ab5d7d"},"outputs":[],"source":["import os, time, requests, urllib, re\n","from bs4 import BeautifulSoup"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":319,"status":"ok","timestamp":1703389457702,"user":{"displayName":"Tomoki Takata","userId":"02827097188944657983"},"user_tz":-540},"id":"Z6u_dVRD6AGL"},"outputs":[],"source":["#あるディレクトリの下層にあるページURLのリストを取得する関数\n","\n","def get_links(url, directory):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    links = []\n","\n","    #1階層下のurlを取得しリストに追加\n","    for link in soup.find_all('a', href=True):\n","        href = link['href']\n","        parsed_href = urllib.parse.urljoin(url, href)\n","        if url in parsed_href and directory in urllib.parse.urlparse(parsed_href).path:\n","            links.append(parsed_href)\n","\n","    # 順序を保持しながら重複を除去\n","    seen = set()\n","    unique_links = []\n","    for link in links:\n","        if link not in seen:\n","            unique_links.append(link)\n","            seen.add(link)\n","\n","    return unique_links"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3577,"status":"ok","timestamp":1703389464841,"user":{"displayName":"Tomoki Takata","userId":"02827097188944657983"},"user_tz":-540},"id":"t8CQdnsQ6AGN","outputId":"65012d86-d53f-418f-9162-38bd7b1ada5c"},"outputs":[],"source":["#日付けごとにフォルダを作成し、テキストと画像をスクレイピングする関数\n","\n","def scrape_and_save_content(url_list, save_directory, delay=0.5):\n","    headers = {'User-Agent': 'Your User Agent Here'}\n","\n","    for url in url_list:\n","        time.sleep(delay)  # サーバーへの負荷を減らすための待機時間\n","        response = requests.get(url, headers=headers)\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        # 日付の取得\n","        date_tag = soup.find('p', class_='date')\n","        date_text = date_tag.get_text() if date_tag else '日付け情報なし'\n","        # 正規表現で日付を抽出し、フォルダ名に適した形式に変換\n","        match = re.search(r'(\\d{4})\\.(\\d{2})\\.(\\d{2})', date_text)\n","        if match:\n","            date_folder = match.group(1) + match.group(2) + match.group(3)  # YYYYMMDD形式\n","        else:\n","            date_folder = '日付け情報なし'\n","\n","        # 店舗情報の取得\n","        shop_tag = soup.find('a', class_='shop')\n","        shop_name = shop_tag.get_text() if shop_tag else '店舗情報なし'\n","        shop_folder = shop_name.replace('店', '')  # \"垂水店\" から \"垂水\" のように変換\n","\n","        # 日付ごと、店舗ごとのフォルダを作成\n","        full_path = os.path.join(save_directory, date_folder, shop_folder)\n","        if not os.path.exists(full_path):\n","            os.makedirs(full_path)\n","\n","        # 文章の保存\n","        text_content = '\\n'.join([p.get_text() for p in soup.find_all('p')])\n","        with open(os.path.join(full_path, f'article_{date_folder}_{shop_folder}.txt'), 'w', encoding='utf-8') as file:\n","            file.write(text_content)\n","\n","        # 上から4つの画像のダウンロードと保存\n","        for i, img in enumerate(soup.find_all('img', src=True)):\n","            if i >= 4:  # 最初の4つの画像のみを対象\n","                break\n","            time.sleep(delay)  # サーバーへの負荷を減らすための待機時間\n","            image_url = img['src']\n","            if not image_url.startswith('http'):\n","                image_url = urllib.parse.urljoin(url, image_url)  # 相対URLを絶対URLに変換\n","            image_response = requests.get(image_url, headers=headers)\n","            if image_response.status_code == 200:\n","                with open(os.path.join(full_path, f'image_{date_folder}_{shop_folder}_{i+1}.jpg'), 'wb') as file:\n","                    file.write(image_response.content)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1703389468350,"user":{"displayName":"Tomoki Takata","userId":"02827097188944657983"},"user_tz":-540},"id":"u09Zvbyx6AGP"},"outputs":[],"source":["#スクレイピングするサイトの設定と確認\n","\n","url = 'ルートディレクトリを選択'\n","directory = '対象のディレクトリを選択'\n","links = get_links(url, directory)\n","\n","print(links)\n","print(f'取得URL数: {len(links)}個')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":679709,"status":"ok","timestamp":1703390152989,"user":{"displayName":"Tomoki Takata","userId":"02827097188944657983"},"user_tz":-540},"id":"YiQR0u1E6AGQ"},"outputs":[],"source":["url_list = links\n","save_directory = '保存先のディレクトリパス'\n","delay = 2  # リクエスト間の待機時間（秒）\n","scrape_and_save_content(url_list, save_directory, delay)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
